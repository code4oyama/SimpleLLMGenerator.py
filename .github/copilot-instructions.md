# 最小LLMプロジェクト

このプロジェクトは、教育目的で作成された最小限の言語モデル（LLM）の実装例です。

## 概要

このプロジェクトには以下が含まれています：
- シンプルなトランスフォーマーベースの言語モデル
- 学習データ
- モデルの訓練スクリプト
- テキスト生成機能

## プロジェクト構成

```
example002/
├── instruction.md          # このファイル
├── requirements.txt        # 必要なパッケージ
├── model.py               # モデルの定義
├── train.py               # 訓練スクリプト
├── generate.py            # テキスト生成スクリプト
└── train_data.txt         # 学習データ
```

## セットアップ

このプロジェクトはPython仮想環境を使用して動作確認を行うことを推奨します。

### 1. Python仮想環境の作成

プロジェクトディレクトリで以下のコマンドを実行して仮想環境を作成します：

```bash
# 仮想環境の作成
python3 -m venv venv

# 仮想環境の有効化（macOS/Linux）
source venv/bin/activate

# 仮想環境の有効化（Windows）
# venv\Scripts\activate
```

仮想環境が有効化されると、コマンドプロンプトの先頭に `(venv)` が表示されます。

### 2. 必要なパッケージのインストール

仮想環境を有効化した状態で、必要なパッケージをインストールします：

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

インストールが完了したら、以下のコマンドでパッケージを確認できます：

```bash
pip list
```

### 3. モデルの訓練

```bash
python train.py
```

訓練が完了すると、`tiny_llm.pth` というファイルが生成されます。

### 4. テキスト生成

```bash
python generate.py
```

### 5. 仮想環境の無効化

作業が終わったら、以下のコマンドで仮想環境を無効化できます：

```bash
deactivate
```

## 動作確認手順

以下の手順で、プロジェクトが正常に動作することを確認できます：

### ステップ1: 環境のセットアップ確認

```bash
# 仮想環境を有効化
source venv/bin/activate

# Pythonバージョンの確認
python --version

# インストール済みパッケージの確認
pip list | grep torch
pip list | grep numpy
```

期待される出力：
- Python 3.8以上
- torch 2.0.0以上
- numpy 1.24.0以上

### ステップ2: モデル訓練の動作確認

```bash
# 訓練を実行（5-10分程度かかります）
python train.py
```

期待される動作：
1. データの読み込みとトークナイザーの初期化
2. モデルパラメータ数の表示
3. 定期的な訓練損失と検証損失の表示
4. `tiny_llm.pth` ファイルの生成
5. 訓練終了時にサンプルテキストの生成

### ステップ3: テキスト生成の動作確認

```bash
# 訓練済みモデルでテキストを生成
python generate.py
```

期待される動作：
1. モデルの読み込み
2. 3つの異なる設定（プロンプトなし、プロンプト付き、低temperature）でテキスト生成
3. 日本語のテキストが生成される（学習データに基づいた内容）

### ステップ4: ファイル生成の確認

訓練後、以下のファイルが生成されていることを確認します：

```bash
ls -lh tiny_llm.pth
```

期待される出力：
- `tiny_llm.pth` ファイルが存在（サイズは約1-2MB程度）

### トラブルシューティング

#### PyTorchのインストールに失敗する場合

```bash
# PyTorchの公式サイトから、環境に合わせたインストールコマンドを確認
# https://pytorch.org/

# 例: CPUのみの環境の場合
pip install torch --index-url https://download.pytorch.org/whl/cpu
```

#### メモリ不足エラーが発生する場合

`train.py` の以下のパラメータを調整してください：
- `batch_size` を小さくする（例: 32 → 16）
- `block_size` を小さくする（例: 64 → 32）

#### 生成されたテキストが意味をなさない場合

これは正常な動作です。このモデルは非常に小さく、学習データも限定的なため、完全に意味のある文章を生成することは期待できません。以下を試すことで改善できます：
- より長い訓練（`max_iters` を増やす）
- より多くの訓練データ
- より大きなモデル（`n_embd`, `n_layer` を増やす）

## モデルの詳細

- **アーキテクチャ**: シンプルなトランスフォーマーデコーダー
- **埋め込み次元**: 128
- **ヘッド数**: 4
- **レイヤー数**: 2
- **コンテキスト長**: 64トークン
- **語彙サイズ**: 訓練データから自動的に構築

## パラメータ

このモデルは非常に小さく、以下のパラメータで構成されています：
- 総パラメータ数: 約50,000-100,000（語彙サイズに依存）
- 訓練時間: CPU上で数分程度

## 注意事項

これは教育目的の最小限の実装です。実用的なアプリケーションには、以下のような改善が必要です：
- より大きなモデルサイズ
- より多くの訓練データ
- より長い訓練時間
- より高度な訓練テクニック（学習率スケジューリング、正則化など）
- GPUの使用

## カスタマイズ

`model.py`の以下のパラメータを調整することで、モデルのサイズを変更できます：
- `n_embd`: 埋め込み次元
- `n_head`: アテンションヘッド数
- `n_layer`: トランスフォーマーレイヤー数
- `block_size`: コンテキスト長

## ライセンス

このコードは教育目的で自由に使用できます。
